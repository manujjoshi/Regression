{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44740f5f",
   "metadata": {},
   "source": [
    "# `Machine Learning`:\n",
    "### 1. `Supervised: mapping function will be learned, (x,y)`\n",
    "    a. Regression: y is continious, here mapping function is y = theta0 + theta1*x\n",
    "    b. Classification: y is categorical\n",
    "### 2. `Unsupervised: learning the pattern from unlabelled data, (x)`\n",
    "    a. Clustering\n",
    "    b. Association Rule Mining\n",
    "    c. Anomaly Detection\n",
    "    d. Dimensionality Reduction\n",
    "### 3. `Reinforcement Learning: follow Rohan Deb lectures`\n",
    "\n",
    "# `Regression Important points and links`\n",
    "- **Supervised learning (SL)** is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.\n",
    "\n",
    "#### - [What is Supervised Learning....wikipedia](https://en.wikipedia.org/wiki/Supervised_learning)\n",
    "\n",
    "- deviation between actual and predicted is called **residue**, ultimate aim of any prediction is to reduce residue.\n",
    "- **Sum of Squared Error(SSE)**: deviation between actual and predicted, (y-y_pred)\\**2\n",
    "- **Maximum error that can happen(SST)**: with one variable SSE = SST, no SSR\n",
    "- But with single variable we cannot reduce error so we go for SST = SSE + SSR, and took two variables that are correlated to predict and draw a line.\n",
    "- y = mx + c (straight line equation), **y = theta1\\*x + theta0**\n",
    "    - y = pred\n",
    "    - m = slope, angle of my line\n",
    "    - x = input variable\n",
    "    - c = intercept, if x=0, y has some value\n",
    "- Here we have to find best **theta0** and  **theta1** to find **best fit line with less residual**.\n",
    "- **least square method(LSM)** = min(sum((y - y_pred)\\*\\*2))\n",
    "    1. Simple Linear Regression: \n",
    "    2. Multiple Linear regression:\n",
    "        - a. Normal equation\n",
    "        - b. Gradient descent approach\n",
    "        - c. Stochastic gradient descent approach\n",
    "        \n",
    "    #### All these methods with a underlying principle of LSM\n",
    "\n",
    "#### - [Linear Regression](https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-your-first-machine-learning-model-linear-regression/)\n",
    "#### - [Your Guide to Linear Regression Models](https://www.kdnuggets.com/2020/10/guide-linear-regression-models.html)\n",
    "#### - [Ordinary Least Squared (OLS) Regression...open in incognito mode](https://medium.com/analytics-vidhya/ordinary-least-squared-ols-regression-90942a2fdad5)\n",
    "- **OLS (Ordinary Least Squared) Regression** is the most simple linear regression model also known as the base model for Linear Regression. While it is a simple model, in Machine learning it is not given much weightage. OLS is one such model which tells you much more than only the accuracy of the overall model. It also tells you how each variables have fared, if we have unwanted variables, if there is autocorrelation in the data and so on.\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQXWqAeQl8gkGEJ5VlmdS-1aT_IY1utnqxDmA&usqp=CAU)\n",
    "#### Note: Random error component cannot be treated/white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40deb3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
